<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Predicting Amazon Reviews</title>
    <meta name="author" content="Harrison Li, Thomas Jiang, Masahiro Kusunoki, Daniel Chen" />
    <meta name="description" content="CS109A Fall 2016 Final Project" />
    <meta name="keywords"  content="" />
    <meta name="Resource-type" content="Document" />

    <link href="https://fonts.googleapis.com/css?family=Lato:300|Lustria" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="css/fullpagemin.css" />
    <link rel="stylesheet" type="text/css" href="css/style.css" />


    <script src="js/jquery.min.js"></script>
    <script src="js/jquery-ui.min.js"></script>

    <script src="js/fullpagemin.js"></script>
    <script src="js/main.js"></script>
</head>
<body>

<ul id="menu">
    <li data-menuanchor="firstPage"><a href="#firstPage">Introduction</a></li>
    <li data-menuanchor="secondPage"><a href="#secondPage">Data</a></li>
    <li data-menuanchor="3rdPage"><a href="#3rdPage">Exploration</a></li>
    <li data-menuanchor="4thPage"><a href="#4thPage">Modeling</a></li>
    <li data-menuanchor="5thPage"><a href="#5thPage">Results</a></li>
    <li data-menuanchor="lastPage"><a href="#lastPage">Conclusion</a></li>
</ul>


<div id="fullpage">

    <div class="section " id="section0">
        <h1>Developing Amazon Recommendations</h1>
        <h3>By Harrison Li, Thomas Jiang, Masahiro Kusunoki, Daniel Chen</h3>
        <h3>CS 109a Fall 2016</h3>
        <input type="hidden" name="id" value="1" />

        <p>The <b>Amazon Fine Foods Dataset</b>, collected and distributed by the Stanford Network Analysis Project, contains nearly <b>600,000 Amazon reviews</b> of over <b>70,000 food and food-related products</b> by over <b>250,000 users</b>. With this data, we set out to develop a system for predicting future ratings based on a combination of <b>product features</b> and <b>user characteristics</b>. If successful, such a system could be used as an accurate recommendation system.</p>
        <!--<img src="imgs/fullPage.png" alt="fullPage" />-->
    </div>

    <div class="section" id="section1">
        <div class="slide">
            <div class="intro">
                <h1>Data Sources:</h1>
                <p>Our data set consists of <b>568,454 reviews</b> on <b>74,258 food-related Amazon products</b> by <b>217468 users</b>. Each review includes UserId, ProductId, Score, and Text, as well as some other values.</p>
                <p>In order to obtain meaningful features for the products in the data set, we took advantage of the Amazon API. This API allowed us to look up items by the ProductIds and return Product Groups. For example, product <b>B003XUL27E</b>, named <b>Healthy Choice All Natural Red Beans & Rice, 14-Ounce Containers (Pack of 6)</b>, returns product group <b>Grocery</b>.</p>
                <p>Click the right arrow to see a full list of features and their explanations.</p>
            </div>
        </div>
        <div class="slide">
            <div class="intro">
                <h2>Datasets</h2>
                <table border="1" align="center" width="800px">
                    <tr>
                        <td colspan="2"><h3>Amazon Fine Foods Dataset</h3></td>
                    </tr>
                    <tr>
                        <td><b>Id</b></td>
                        <td>Unique number assigned to each review</td>
                    </tr>
                    <tr>
                        <td><b>ProductId</b></td>
                        <td>Unique identifier for the product</td>
                    </tr>
                    <tr>
                        <td><b>UserId</b></td>
                        <td>Unique identifier for the user</td>
                    </tr>
                    <tr>
                        <td><b>ProfileName</b></td>
                        <td>User name corresponding to the UserId</td>
                    </tr>
                    <tr>
                        <td><b>HelpfulnessNumerator</b></td>
                        <td>Number of users who rated the review as helpful</td>
                    </tr>
                    <tr>
                        <td><b>HelpfulnessDenominator</b></td>
                        <td>Number of users who indicated whether they found the review helpful</td>
                    </tr>
                    <tr>
                        <td><b>Score</b></td>
                        <td>Rating between 1 and 5</td>
                    </tr>
                    <tr>
                        <td><b>Time</b></td>
                        <td>Timestamp for the review</td>
                    </tr>
                    <tr>
                        <td><b>Summary</b></td>
                        <td>Summary of review, written by user</td>
                    </tr>
                    <tr>
                        <td><b>Text</b></td>
                        <td>full review text</td>
                    </tr>
                </table>
                <br>
                <table border="1" align="center" width="800px">
                    <tr>
                        <td colspan="2"><h3>Amazon API</h3></td>
                    </tr>
                    <tr>
                        <td><b>Product ID</b></td>
                        <td>Input provided from Amazon Fine Food Dataset</td>
                    </tr>
                    <tr>
                        <td><b>Product Group</b></td>
                        <td>Categorization of product defined by Amazon</td>
                    </tr>
                </table>
            </div>
        </div>
    </div>

    <div class="section" id="section2">
        <div class="slide">
            <div class="intro">
                <h1>Data Exploration</h1>
                <p>As with any real world dataset, the Amazon Fine Foods Dataset contained many peculiarities. We present six of the most interesting observations: <b>Duplicates</b>, <b>Sparsity by User</b>, <b>Sparsity by Product</b>, <b>Rarity of Helpfulness Scores</b>, <b>Disproportional Score Distributions</b>, and <b>Time Drifts</b>. We corrected for the first four observations during our modeling. Though the remaining two did not inform our model, we present them as curiosities that would be interesting to explore in future work.</p>
                <p>Click the right arrow to see individual observations from our exploration.</p>
            </div>
        </div>
        <div class="slide">
            <div class="intro">
                <h2>1. Duplicates</h2>
                <img src="img/duplicates.png" width="300px">
                <p>While exploring reviews, we found that several reviews had identical text, scores, user IDs, and timestamps on similar products. This does not seem to suggest a human user, and we hypothesize that these reviewers were left by bots. Including these bots would improve our RMSE (as bots are extremely easy to predict since they continually give identical scores), but for the sake of applicability to human users, we decided to filter out all such reviews, eliminating almost half of all reviews.</p>
            </div>
        </div>
        <div class="slide">
            <div class="intro">
                <h2>2. Number of Reviews by Reviewer</h2>
                <img src="img/numreviews.png" height="330px"><img src="img/numreviewsperusers.png" height="330px">
                <p>This compares the number of active users, which we define to be users with 5 or more ratings, to the number of inactive users. Inactive users do not have enough ratings for us to accurately judge their product preferences, and therefore our model also removes them when training on our dataset.</p>
            </div>
        </div>
        <div class="slide">
            <div class="intro">
                <h2>3. Number of Reviews by Product</h2>
                <img src="img/numreviewsforprod.png" height="330px"><img src="img/numreviewsperprod.png" height="330px">
                <p>Compounding the sparsity of user ratings, each product also generally has very few ratings. With so few reviews, it's difficult to accurately judge product similarity using just the Fine Food Dataset, motivating us to use the Amazon API to provide additional product features.</p>
            </div>
        </div>
        <div class="slide">
            <div class="intro">
                <h2>4. Helpfulness Skew</h2>
                <img src="img/helpfulness.png" width="600px">
                <p>The data provides both helpfulness numerator and denominator. The ratio of the two gives the percent of readers who find each review to be helpful. It's clear that once again, most reviews have very few helpfulness ratings. A rating of 1 out of 1 or 0 out of 1 is not dramatically significant, so when using helpfulness as a proxy for estimating review quality, we assigned a 1, 1 beta prior.</p>
            </div>
        </div>
        <div class="slide">
            <div class="intro">
                <h2>5. Score Skew</h2>
                <img src="img/scoreswodup.png" height="330px"><img src="img/scorespie.png" height="330px">
                <p>Interestingly, a large majority (63.9%) of the ratings given by users across all products are 5 out of 5, while negative ratings (defined as 3 stars or less) constitute only 22% of all ratings. We did not correct for this phenomenon since we treat the scores as quantitative values instead of categorical, but it is reflected by predicted values also yielding a high average.</p>
            </div>
        </div>
        <div class="slide">
            <div class="intro">
                <h2>6. Time Drift</h2>
                <img src="img/time.png" height="400px">
                <p>Scores seem to improve over time throughout the dataset. We do not yet have a robust explanation of this phenomenon, but it could potentially be accounted for by a more complex model.</p>
            </div>
        </div>

        <!--<div class="slide">-->
            <!--<div class="intro">-->
                <!--<h1>How People Review Products</h1>-->
                <!--<p>First, we explored distributions of ratings. This allowed us to learn how sparse the data set was. A data set where every user reviewed a majority of the products and each product would have a large number of reviews would be ideal for modeling, but this will turn out not to be the case. Instead, this is an informative method for understanding what data will have to be filtered out and what data can be used directly from the data set.</p>-->
            <!--</div>-->
        <!--</div>-->
        <!--<div class="slide">-->
            <!--<div class="intro">-->
                <!--<h2>Score Distribution</h2>-->
                <!--<img src="img/scores.png">-->
                <!--<p>Interestingly, a large majority (63.9%) of the ratings given by users across all products are 5 out of 5, while negative ratings (defined as 3 stars or less) constitute only 22% of all ratings.  The heavy skew towards 5 star ratings might present a challenge in distinguishing between products or users.-->
                <!--</p>-->
                <!--<table border="1" align="center">-->
                    <!--<tr>-->
                        <!--<td>-->
                            <!--<b>Score:</b>-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--1-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--2-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--3-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--4-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--5-->
                        <!--</td>-->
                    <!--</tr>-->
                    <!--<tr>-->
                        <!--<td>-->
                            <!--<b>Percent:</b>-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--9.19%-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--5.24%-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--7.50%-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--14.19%-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--63.88%-->
                        <!--</td>-->
                    <!--</tr>-->
                <!--</table>-->
            <!--</div>-->
        <!--</div>-->
        <!--<div class="slide">-->
            <!--<div class="intro">-->
                <!--<h2>User Activity</h2>-->
                <!--<img src="img/numreviews.png">-->
                <!--<p>Breaking down the users by number of reviews, it's clear that the vast majority of users review very few products. In fact, out of the 256059 unique users, over half of them only have one review! Since it is impossible to create a robust model with only a single data point for each user, we will need to filter for "active users", which we will define as users with greater than _____ reviews.</p>-->
                <!--<table border="1" align="center">-->
                    <!--<tr>-->
                        <!--<td>-->
                            <!--<b>Metric:</b>-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--Min Num of Reviews-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--First Quartile-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--Median-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--Third Quartile-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--Max Num of Reviews-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--Mean-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--Count-->
                        <!--</td>-->
                    <!--</tr>-->
                    <!--<tr>-->
                        <!--<td>-->
                            <!--<b>Value:</b>-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--1-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--1-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--1-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--2-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--448-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--2.22-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--256059-->
                        <!--</td>-->
                    <!--</tr>-->
                <!--</table>-->
            <!--</div>-->
        <!--</div>-->
        <!--<div class="slide">-->
            <!--<div class="intro">-->
                <!--<h2>Products by Number of Reviews</h2>-->
                <!--<img src="img/numreviewsforprod.png">-->
                <!--<p>Once again, we see that the majority of products have only been reviewed a small number of times. Out of the 74258 unique products, half have been reviewed two or fewer times. This indicates that we cannot simply use each product's score to assign a quality to it. Instead, we will either have to filter out products with too few reviews or use a prior to properly score products based on user ratings.</p>-->
                <!--<table border="1" align="center">-->
                    <!--<tr>-->
                        <!--<td>-->
                            <!--<b>Metric:</b>-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--Min Num of Reviews-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--First Quartile-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--Median-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--Third Quartile-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--Max Num of Reviews-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--Mean-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--Count-->
                        <!--</td>-->
                    <!--</tr>-->
                    <!--<tr>-->
                        <!--<td>-->
                            <!--<b>Value:</b>-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--1-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--1-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--2-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--3-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--913-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--7.66-->
                        <!--</td>-->
                        <!--<td>-->
                            <!--74258-->
                        <!--</td>-->
                    <!--</tr>-->
                <!--</table>-->
            <!--</div>-->
        <!--</div>-->
        <!--<div class="slide">-->
            <!--<div class="intro">-->
                <!--<h1>Text Analysis of Reviews</h1>-->
                <!--<p>We also did some exploration of the actual text left by each user. Ideally, we would like to use text reviews in potentially two ways. We can try to model similarities between customers based on their word choices or extract similarities between products based on what people say about them.</p>-->
            <!--</div>-->
        <!--</div>-->
        <!--<div class="slide">-->
            <!--<div class="intro">-->
                <!--<h2>Word Count</h2>-->
                <!--<img src="img/wordcount.png">-->
            <!--</div>-->
        <!--</div>-->
    </div>

    <div class="section" id="section3">
        <div class="slide">
            <div class="intro">
                <h1>Modeling</h1>
                <p>
                    We developed our final model in three steps. First, we created <b>two baseline models to serve as our target to beat</b>. Then, we developed a model for determining a <b>product's "true" score based on previous reviews by other users</b>. Finally, we developed a method for <b>calculating product similarity by review text</b>, which we were then able to use to <b>estimate user preferences</b>.</p>
                <p>For modeling, the dataset, after all corrections discussed in the previous section, was split into a 70% training set and 30% testing set.</p>
                    <p>Click the right arrow to see how we developed our final model.</p>

            </div>
        </div>
        <div class="slide">
            <div class="intro">
                <h2>Baseline Models</h2>
                <p>
                    We developed two baseline models, derived from a simple assumption. Our baseline models assume users will vote according to their past average. Thus, both of our baseline models simply predict future ratings by assigning them the mean or median of the user's past reviews.</p>
                <table border="1" align="center" width="800px">
                    <tr>
                        <td><b>Model</b></td>
                        <td><b>RMSE</b></td>
                    </tr>
                    <tr>
                        <td><b>User Median Model</b></td>
                        <td>1.289</td>
                    </tr>
                    <tr>
                        <td><b>User Mean Model</b></td>
                        <td>1.198</td>
                    </tr>
                </table>
            </div>
        </div>
        <div class="slide">
            <div class="intro">
                <h2>Improvement</h2>
                <p>Our final content-based model was a combination of a <b>product-based component</b> and a <b>user-based component</b>, with the relative weightings of the two components controlled by the parameter β.</p>
                <p>The <b>product-based component</b> will judge the general quality of each product, with the assumption that in general, people <b>prefer objectively higher quality products</b> to generally poor quality products, even if they don't have a specific preference for the item's features. To achieve this evaluation, we used a weighted average of all the scores of each item, where the assigned weight of each review was based on the helpfulness of the review (assigned using an uninformative uniform prior).</p>
                <p>The <b>user-based component</b> assumes that <b>users will prefer products that are similar to other products that they've both purchased and rated well</b>. Judging product similarity was a nontrivial task. Because our dataset had minimal information on the specific products, we decided to combine two methods to generate product similarity. First, for each product, we aggregated all of its text reviews and vectorized it using a TF-IDF vectorizer (which converts text in a document to a vector of word counts weighted inversely by the overall frequency of words across all documents) and obtained its cosine similarity with all other products. Then, we used the Amazon API to extract product groups. Products that were in the same product groups had their similarity inflated by a tunable variable α.
                </p>
                <p>To see a more mathematically rigorous explanation, please click the right arrow.</p>
            </div>
        </div>
        <div class="slide">
            <div class="intro">
                <!--<h2>Product-based Component</h2>-->
                <img src="img/product.png" class="component">
                <p>
                    The product-based component is a weighted average of all ratings of that product in the training set, where each rating is weighed by the estimated proportion of users who rated user <i>i</i>’s rating on product <i>j</i> as helpful. This expression is derived from placing an uninformative Uniform (i.e. Beta(1,1)) prior on the helpfulness and taking the posterior mean (PM) estimate after updating the prior with the helpfulness numerator and denominator.
                </p>
            </div>
        </div>
        <div class="slide">
            <div class="intro">
                <!--<h2>User-based Component</h2>-->
                <img src="img/user.png" class="component">
                <p>
                    The user-based component is a weighted average of all of the ratings given by a particular user, where a similarity score between 0 and 1 for products <i>n</i> and <i>j</i>, represents the text similarity of the aggregated reviews of products <i>n</i> and <i>j</i> calculated using a TF-IDF vectorizer and cosine similarity. Due to computational limitations we only computed similarity scores for the 10,000 most reviewed products in the training set.</p>

                <p>Product group is a categorical designation assigned by Amazon to each product (e.g. “grocery”, “pet foods”). The parameter α controls how much the similarity score for products in the same product group is inflated towards 1. Note that if α = 1, then product similarity reduces to text similarity in all cases.
                </p>
            </div>
        </div>
        <div class="slide">
            <div class="intro">
                <!--<h2>Final Model</h2>-->
                <img src="img/final.png" class="component">
                <p>
                    Our final model is the sum of the product-based and user-based components with relative weights assigned.
                </p>
            </div>
        </div>
    </div>

    <div class="section" id="section4">
        <div class="slide">
            <div class="intro">
                <h1>Results</h1>
                <p>We take the final model developed above, and tune the weighting parameters (α and β) to find our optimal model. We use KFold Validation on the original training dataset, we compute and plot RMSE scores to understand the impacts of the different weights. We then apply our optimal model onto the testing dataset.</p>
                <p>Click the right arrow to see our tuning process and final results.
                </p>
            </div>
        </div>
        <div class="slide">
            <div class="intro">
                <h3>Tuning</h3>
                <img src="img/tuning.png" id="tuning">
                <p>We tune the α and β parameters of the model by using KFold Validation (K=5). Plotting the RMSE scores, we see that this is more sensitive to changes in β, while α seems to have a small effect. We choose the optimal scores with  α = 1 and β = 0.9

                </p>
            </div>
        </div>
        <div class="slide">
            <div class="intro">
                <h3>Improvement over Baseline</h3>
                <img src="img/rmsered.png" id="rmsered">
                <p>As shown above, our final model manages to beat both of our baseline models by customizing our predictions based on a combination of user preferences for each product, as well as the inherent quality of the products being reviewed.</p>
            </div>
        </div>
    </div>

    <div class="section" id="section5">

        <div class="slide">
            <div class="intro">
                <h1>Conclusions</h1>
                <p>We were successfully able to beat our baseline models. However, the final tuning of our model raises several questions that should be addressed in the future. For example, the selection of α = 1 indicates that <b>product groups do not contribute meaningfully to our model</b>. This may has two possible explanations. The first is that several of the Product IDs were no longer recognized recognized by the Amazon API, which suggests that these products are no longer being sold. This makes sense given that the dataset contains reviews from <b>Oct 1999 - Oct 2012</b>. The second is that the product groups do not represent an accurate method for categorizing products. Manually checking these values supports this. For example, when examining coconut oils from different brands, one was grouped under <b>Groceries</b> while the other was under <b>Health and Beauty</b>.</p>
                <p>Click the right arrow to see our suggestions for future work.</p>
            </div>
        </div>
        <div class="slide">
            <div class="intro">
                <h2>Future Work</h2>
                <p>Though we beat our baseline models, our model only performs marginally better than a raw weighting of product mean and user mean. It appears that the similarity matrix was not an effective tool in our model. Furthermore, it suggests that user preferences, as far as we can tell, are not as individual as we believed, simply determining the benchmark the user reviews based on.</p>
                <p>Furthermore, due to computational limitations, we were limited in the amount of products and reviews that we could predict on. Given the funding and computation time, it might be worth rerunning the entire computation process on the complete dataset.</p>
                <p>Additionally, more features on users and products would give more information to train the model's components. This may be difficult given that Amazon’s API does not have an easy feature generation process for items and finding individual users and collecting more reviews is difficult due to privacy concerns. However, doing so may increase the amount of data to train on as well as improve the similarity matrix calculation process.</p>
                <p>Finally, in our model, we currently disregard the timestamps of reviews. As shown in the Data Exploration section, we noticed a time drift in scores, but it is unclear what sort of predictive power or interpretation this feature would have in a more complicated final model, but it may be useful in normalize user ratings over time when calculating the product based components.</p>
                <p>Click the right arrow to see our references.</p>
            </div>
        </div>
        <div class="slide">
            <div class="intro">
                <h2>References</h2>
                <p>Das, S. (2015, August 11). Beginners Guide to learn about Content Based Recommender Engines. Retrieved December 08, 2016, from https://www.analyticsvidhya.com/blog/2015/08/beginners-guide-learn-content-based-recommender-systems</p>
                <p>Marafi, S. (2015, April 28). Collaborative Filtering with Python. Retrieved December 08, 2016, from http://www.salemmarafi.com/code/collaborative-filtering-with-python/</p>
                <p>Koren, Y. (2009). The bellkor solution to the netflix grand prize. Netflix prize documentation, 81, 1-10.</p>
                <p>Leskovec, J., Rajaraman, A., & Ullman, J. D. (2014). Mining of massive datasets / Jure Leskovec, Standford University, Anand Rajaraman, Milliways Labs, Jeffrey David Ullman, Standford University (2nd ed.). Cambridge: Cambridge University Press.</p>
            </div>
        </div>
    </div>


</div>

</body>
</html>
